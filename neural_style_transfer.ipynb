{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_style_transfer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrr923KeJd8cJTMA1xVyZ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qJWZmMiM3O_6","colab_type":"text"},"source":["***Copyright 2019 Pätzold, Menzel, Zacharias.***\n"]},{"cell_type":"code","metadata":{"id":"vAQ4g_Qn2qXk","colab_type":"code","colab":{}},"source":["#Licensed under the Apache License, Version 2.0 (the \"License\");\n","#you may not use this file except in compliance with the License.\n","#You may obtain a copy of the License at\n","#\n","#    http://www.apache.org/licenses/LICENSE-2.0\n","#\n","#Unless required by applicable law or agreed to in writing, software\n","#distributed under the License is distributed on an \"AS IS\" BASIS,\n","#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","#See the License for the specific language governing permissions and\n","#limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VrRO-BPM5q3","colab_type":"text"},"source":["# **Neural Algorithm of Artistic Style**\n","\n","Here we implement the network which enables us to combine two separate images,  \n","namely one image providing the content and the other one adding the style to this content image.  \n","For this purpose, we use a convolutional net - the VGG19 - and build a new feature space on top to reconstruct the style of the image.\n","<br />\n","The code partially uses code and structure from the TensorFlow tutorial: Neural Style Transfer (2018) and the ANN is replicated after the study from Gatys et al. (2015)."]},{"cell_type":"markdown","metadata":{"id":"D3bqFl4rBHWS","colab_type":"text"},"source":["***Basic idea:***\n","\n","Two images are input to the neural network: A content-image and a style-image. We wish to generate the mixed-image which has the contours of the content-image and the colours and texture of the style-image. We do this by creating several loss-functions that can be optimized.\n","\n","The loss-function for the content-image tries to minimize the difference between the features that are activated for the content-image and for the mixed-image, at one or more layers in the network. This causes the contours of the mixed-image to resemble those of the content-image.\n","\n","The loss-function for the style-image is slightly more complicated, because it instead tries to minimize the difference between the so-called Gram-matrices for the style-image and the mixed-image. This is done at one or more layers in the network. The Gram-matrix measures which features are activated simultaneously in a given layer. Changing the mixed-image so that it mimics the activation patterns of the style-image causes the colour and texture to be transferred.\n","\n","We use TensorFlow to automatically derive the gradient for these loss-functions. The gradient is then used to update the mixed-image. This procedure is repeated a number of times until we are satisfied with the resulting image."]},{"cell_type":"markdown","metadata":{"id":"BNpMnSIIb5xQ","colab_type":"text"},"source":["*Import all necessary packages.*"]},{"cell_type":"code","metadata":{"id":"2notn6MGM0MX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1584879524581,"user_tz":-60,"elapsed":2540,"user":{"displayName":"Florian Pätzold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyRtwRIkffJZpBVuR5kbtWnF4-I9PCy9lJj7YQQA=s64","userId":"14560267868897327267"}},"outputId":"3477a637-2d3d-40f3-b56d-6d5d8725d453"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import PIL.Image\n","import numpy as np"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dfOKoWTxM_ly","colab_type":"text"},"source":["### *Image manipulation*\n","In order to load new input images, save mixed (output) images with transferred style and to plot the results during the transformation process, we define some important convenience functions."]},{"cell_type":"markdown","metadata":{"id":"WZKcLa6bbmf5","colab_type":"text"},"source":["*Loading an image.*"]},{"cell_type":"code","metadata":{"id":"YdEVsZHRM-xY","colab_type":"code","colab":{}},"source":["def load_image(filename, filepath):\n","\n","  \"\"\"\n","  filename: The name of the image (behind backslash, before file type)\n","  filepath: The URL path of the image.\n","  \"\"\"\n","\n","  # Read in the image.\n","  path = tf.keras.utils.get_file(filename, filepath)\n","  img = tf.io.read_file(path)\n","  # Preprocess image: decode and convert to float.\n","  img = tf.image.decode_image(img, channels=3)\n","  img = tf.image.convert_image_dtype(img, tf.float32)\n","  # Scale the image if any dimension is larger than 512 pixels.\n","  max_dim = 512\n","  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n","  long_dim = max(shape)\n","  scale = max_dim / long_dim\n","  \n","  new_shape = tf.cast(shape * scale, tf.int32)\n","  img = tf.image.resize(img, new_shape)\n","  img = img[tf.newaxis, :]\n","  return img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amK-DMrnbsVJ","colab_type":"text"},"source":["*Transform tensor into image.*"]},{"cell_type":"code","metadata":{"id":"C7xT02Jfbxd3","colab_type":"code","colab":{}},"source":["def tensor_to_image(tensor):\n","\n","  \"\"\"\n","  tensor: Tensor input to be converted to an image.\n","  \"\"\"\n","\n","  # To get the image we turn the tensor into an uint8 array in [0,255].\n","  tensor = tensor*255\n","  tensor = np.array(tensor, dtype=np.uint8)\n","  # The image must have no more than 3 dimensions.\n","  if np.ndim(tensor)>3:\n","    assert tensor.shape[0] == 1\n","    tensor = tensor[0]\n","  return PIL.Image.fromarray(tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ar-MhEqg30J","colab_type":"text"},"source":["*Plot content and style image.*"]},{"cell_type":"code","metadata":{"id":"O3EvnBdtg6Vq","colab_type":"code","colab":{}},"source":["def plot_images(content_image, style_image, mixed_image):\n","\n","  \"\"\"\n","  content_image: The image that is providing the contours for the mixed image.\n","  style_image: The image that is providing colour and structure for the mixed image.\n","  mixed_image: The resulting mixed image.\n","  \"\"\"\n","\n","  # Create figure with sub-plots.\n","  fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n","  images = [content_image, style_image, mixed_image]\n","  labels = [\"content\", \"style\", \"mixed\"]\n","\n","  # Label and plot every image.\n","  for i in range(3):\n","    img = images[i]\n","    lbl = labels[i]\n","    # If shape of the image is too big it cannot be plotted.\n","    if len(img.shape) > 3:\n","      img = tf.squeeze(img, axis=0)\n","    img = tf.image.resize(img, (512,512))\n","    ax[i].imshow(img, interpolation='sinc')\n","    ax[i].set_title(lbl)\n","    ax[i].axis(\"off\")\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJjOFe49b0eG","colab_type":"text"},"source":["*Save an image.*"]},{"cell_type":"code","metadata":{"id":"21I6XNXdb2eQ","colab_type":"code","colab":{}},"source":["def save_image(image, filename='mixed_image.jpeg'):\n","\n","  \"\"\"\n","  image: The image to be saved.\n","  filename: The name you want to save your image with.\n","  \"\"\"\n","\n","  # Add .jpeg (in default filename) and save in colab files\n","  image.save(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWRgAVLqbWY6","colab_type":"text"},"source":["*Load the data*"]},{"cell_type":"code","metadata":{"id":"QB7cDHgfbb_X","colab_type":"code","colab":{}},"source":["# Load a content and a style image.\n","content_image = load_image('YellowLabradorLooking_new','https://upload.wikimedia.org/wikipedia/commons/2/26/YellowLabradorLooking_new.jpg')\n","style_image = load_image('04-Pablo-Picasso-Head-of-Woman-Fernande-1909-56a03c7f5f9b58eba4af7614', 'https://www.thoughtco.com/thmb/DUsvOYMGSDG1LjjME0TfV3XgJO4=/2178x1800/filters:no_upscale():max_bytes(150000):strip_icc()/04-Pablo-Picasso-Head-of-Woman-Fernande-1909-56a03c7f5f9b58eba4af7614.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TL0tn2NyNSE8","colab_type":"text"},"source":["### *Build the model*\n","Here we implement the main model which is used for the optimization process of the mixed image."]},{"cell_type":"code","metadata":{"id":"hU7HzvunNUtl","colab_type":"code","colab":{}},"source":["class StyleContentModel(tf.keras.models.Model):\n","  def __init__(self, style_layers, content_layers):\n","    super(StyleContentModel, self).__init__()\n","    self.vgg = vgg_layers(style_layers + content_layers)\n","    self.style_layers = style_layers\n","    self.content_layers = content_layers\n","    self.num_style_layers = len(style_layers)\n","    # Set the model as non-trainable.\n","    self.vgg.trainable = False\n","\n","  # Compute forward step.\n","  def call(self, inputs):\n","\n","    \"\"\" Expects float input in [0,1]. \"\"\"\n","\n","    inputs = inputs*255.0\n","\n","    # Get the outputs of a forward step and save in output lists.\n","    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","    outputs = self.vgg(preprocessed_input)\n","    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n","                                      outputs[self.num_style_layers:])\n","    \n","    # For the style outputs we transform each element into a gram matrix.\n","    style_outputs = [gram_matrix(style_output)\n","                     for style_output in style_outputs]\n","\n","    # Create feed dictionaries for content and style.\n","    content_dict = {content_name:value \n","                    for content_name, value \n","                    in zip(self.content_layers, content_outputs)}\n","    style_dict = {style_name:value\n","                  for style_name, value\n","                  in zip(self.style_layers, style_outputs)}\n","    \n","    return {'content':content_dict, 'style':style_dict}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lIZ0fkwtNajR","colab_type":"text"},"source":["The style of an image can be described by the means and correlations across the different feature maps. We calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations."]},{"cell_type":"markdown","metadata":{"id":"FoJ8_iZtAXOf","colab_type":"text"},"source":["*Transform tensors for the style layers into gram matrices.*"]},{"cell_type":"code","metadata":{"id":"rewdCelfNdIX","colab_type":"code","colab":{}},"source":["def gram_matrix(tensor):\n","\n","  \"\"\"\n","  tensor: Tensor input to be transformed into a gram matrix.\n","  \"\"\"\n","  \n","  # Get the tensor's shape.\n","  shape = tf.shape(tensor)\n","  # Get the number of feature channels for the input tensor,\n","  # which is assumed to be from a convolutional layer with 4-dim.\n","  num_channels = int(shape[3])\n","  # Reshape the tensor so it is a 2-dim matrix. This essentially\n","  # flattens the contents of each feature-channel.\n","  matrix = tf.reshape(tensor, shape=[-1, num_channels])  \n","  # Calculate the Gram-matrix as the matrix-product of\n","  # the 2-dim matrix with itself. This calculates the\n","  # dot-products of all combinations of the feature-channels.\n","  gram = tf.matmul(tf.transpose(matrix), matrix)\n","\n","  return gram"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCOz5THZNgOZ","colab_type":"text"},"source":["*Calculate intermediate layer outputs.*"]},{"cell_type":"code","metadata":{"id":"BlOH7zxgNgvN","colab_type":"code","colab":{}},"source":["def vgg_layers(layer_names):\n","\n","  \"\"\" \n","  Creates a vgg model that returns a list of intermediate output values.\n","  layer_names: The names of the layer for which we compute the outputs.\n","  \"\"\"\n","  \n","  # Load our model. Load pretrained VGG, trained on imagenet data.\n","  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","  vgg.trainable = False\n","  # Compute the output for each layer.\n","  outputs = [vgg.get_layer(name).output for name in layer_names]\n","  # Define a model using functional API.\n","  model = tf.keras.Model([vgg.input], outputs)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-IzdtIzNmDI","colab_type":"text"},"source":["In order to get the contour features of the content image and the colour and texture of the style image, we need to calculate different losses for each layer. This function calculates the mean squared error between the feature activations of content and mixed image for the chosen layers. If the loss between these two images would be zero, the content image would have perfectly transferred the contour features onto the resulting mixed image."]},{"cell_type":"markdown","metadata":{"id":"oOoY17HUcUz3","colab_type":"text"},"source":["*Content and style loss.*"]},{"cell_type":"code","metadata":{"id":"6SPN0QVQNxva","colab_type":"code","colab":{}},"source":["# Define content and style layers from which we pull feature maps.\n","content_layers = ['block5_conv2']\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1']\n","num_content_layers = len(content_layers)\n","num_style_layers = len(style_layers)\n","\n","def style_content_loss(outputs, style_weight=1e-2, content_weight=1e4):\n","\n","  \"\"\"\n","  Calculates the loss between targets and layer outputs.\n","  outputs: The list with outputs for each layer.\n","  style_weight: The weight of the style image.\n","  content_weight: The weight of the content image.\n","  \"\"\"\n","\n","  # Get style and content outputs.\n","  style_outputs = outputs['style']\n","  content_outputs = outputs['content']\n","  # Calculate weighted style loss by averaging over all style losses.\n","  style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n","                         for name in style_outputs.keys()])\n","  style_loss *= style_weight / num_style_layers\n","  # Calculate weighted content loss by averaging over all content losses.\n","  content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n","                           for name in content_outputs.keys()])\n","  content_loss *= content_weight / num_content_layers\n","  # Calculate total loss.\n","  loss = style_loss + content_loss\n","  \n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuF4V4-SN0f7","colab_type":"text"},"source":["### *Optimization*"]},{"cell_type":"markdown","metadata":{"id":"ma3UNAm7cmzQ","colab_type":"text"},"source":["*Initialization and instantiation.*"]},{"cell_type":"code","metadata":{"id":"aCdoRYGRN1cP","colab_type":"code","colab":{}},"source":["# Initialize Adam optimizer.\n","opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n","\n","# Instantiate the model/extractor.\n","extractor = StyleContentModel(style_layers, content_layers)\n","\n","# Targets.\n","style_targets = extractor(style_image)['style']\n","content_targets = extractor(content_image)['content']\n","\n","# Input of same shape as content_image.\n","mixed_image = tf.Variable(content_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hk14gYducvYv","colab_type":"text"},"source":["*Update function.*"]},{"cell_type":"code","metadata":{"id":"xnLAhiP-cuio","colab_type":"code","colab":{}},"source":["@tf.function()\n","def train_step(image, total_variation_weight=30):\n","\n","  \"\"\"\n","  Computes one training step.\n","  image: The to be optimized mixed image.\n","  total_variation_weight: Weight of the total variation loss to reduce high frequency artifacts.\n","  \"\"\"\n","\n","  # Start a gradient type for gradient descent.\n","  with tf.GradientTape() as tape:\n","    # Get outputs and losses.\n","    outputs = extractor(image)\n","    loss = style_content_loss(outputs)\n","    # Total Variation denoising:\n","    # Average of differences in images when they are shifted by a pixel on x- and y-axis.\n","    loss += total_variation_weight*tf.image.total_variation(image)\n","\n","  # Apply gradients and optimize the image.\n","  grad = tape.gradient(loss, image)\n","  opt.apply_gradients([(grad, image)])\n","  # Make sure the image values stay in [0,1].\n","  image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cxp9VOzoc29_","colab_type":"text"},"source":["*Main function to transfer style of an image.*"]},{"cell_type":"code","metadata":{"id":"8PX26FTkc9dw","colab_type":"code","colab":{}},"source":["def transfer_style(image=mixed_image, steps=500):\n","\n","  \"\"\"\n","  The main function to transfer the style onto the content image.\n","  image: The resulting mixed image which gets optimized.\n","  steps: The number of training steps. At least 500 is recommended.\n","  \"\"\"\n","\n","  # Initialize step counter.\n","  i = 0\n","  # Perform 'steps' training steps and plot intermediate results.\n","  while i < steps:\n","    train_step(image)\n","    i += 1\n","    if i%10 == 0 or i==1:\n","      if i%50 == 0 or i<=50:\n","        print()\n","        print()\n","        print(\"Iterations: \", i)\n","        plot_images(content_image, style_image, image)\n","\n","  print()\n","  print()\n","  print(\"Resulting mixed image after \", i, \" iteration(s):\")\n","\n","  # Save the resulting mixed image to colab files.\n","  save_image(tensor_to_image(image))\n","\n","  # Plot the resulting mixed image.\n","  plt.figure(figsize=(7,7))\n","  if len(image.shape) > 3:\n","    image = tf.squeeze(image, axis=0)\n","  plt.imshow(image)\n","  plt.axis(\"off\")\n","  plt.title(\"mixed\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UPibIWykN6Ey","colab_type":"text"},"source":["### *Example*"]},{"cell_type":"code","metadata":{"id":"ySF1EqTAOJpg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"188sIpi9zpUFeHdurSR1c6WQJFcWuMCSy"},"executionInfo":{"status":"ok","timestamp":1584879606172,"user_tz":-60,"elapsed":83972,"user":{"displayName":"Florian Pätzold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyRtwRIkffJZpBVuR5kbtWnF4-I9PCy9lJj7YQQA=s64","userId":"14560267868897327267"}},"outputId":"5adb44a5-4908-41bf-835d-6daa2c0f728b"},"source":["# Hint: Run on GPU (Runtime > Change Runtime Type > Hardware Accelerator > GPU) for faster style transfer!\n","transfer_style(steps=1000)\n","\n","# You can find a downloaded version of the \n","# resulting mixed image in your colab files folder."],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"5F00_Xst4lqn","colab_type":"text"},"source":["### References\n","Gatys, L. A., Ecker, A. S., & Bethge, M. (2015). A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576. \n","<br />\n","Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Neural Style Transfer, (2018). Software available from tensorflow.org."]}]}